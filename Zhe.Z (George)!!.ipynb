{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8 (default, Oct 14 2019, 21:22:53) \n",
      "[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /home/hadoop/.local/lib/python2.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib64/python2.7/site-packages (from seaborn) (1.14.5)\n",
      "Requirement already satisfied: pandas>=0.17.1 in /home/hadoop/.local/lib/python2.7/site-packages (from seaborn) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.17.1 in /home/hadoop/.local/lib/python2.7/site-packages (from seaborn) (1.2.3)\n",
      "Requirement already satisfied: matplotlib>=1.5.3 in /home/hadoop/.local/lib/python2.7/site-packages (from seaborn) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/hadoop/.local/lib/python2.7/site-packages (from pandas>=0.17.1->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /home/hadoop/.local/lib/python2.7/site-packages (from pandas>=0.17.1->seaborn) (2019.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib>=1.5.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib>=1.5.3->seaborn) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib>=1.5.3->seaborn) (1.14.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib>=1.5.3->seaborn) (2.4.7)\n",
      "Requirement already satisfied: subprocess32 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib>=1.5.3->seaborn) (3.5.4)\n",
      "Requirement already satisfied: backports.functools-lru-cache in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib>=1.5.3->seaborn) (1.6.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python2.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.3->seaborn) (36.2.7)\n",
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/hadoop/.local/lib/python2.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib64/python2.7/site-packages (from matplotlib) (1.14.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib) (1.14.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: subprocess32 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib) (3.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: backports.functools-lru-cache in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib) (1.6.1)\n",
      "Requirement already satisfied: pytz in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib) (2019.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python2.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (36.2.7)\n",
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plotly==4.6.0 in /home/hadoop/.local/lib/python2.7/site-packages (4.6.0)\n",
      "Requirement already satisfied: six in /home/hadoop/.local/lib/python2.7/site-packages (from plotly==4.6.0) (1.14.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /home/hadoop/.local/lib/python2.7/site-packages (from plotly==4.6.0) (1.3.3)\n",
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/hadoop/.local/lib/python2.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/hadoop/.local/lib/python2.7/site-packages (from sklearn) (0.20.4)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib64/python2.7/site-packages (from scikit-learn->sklearn) (1.14.5)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /home/hadoop/.local/lib/python2.7/site-packages (from scikit-learn->sklearn) (1.2.3)\n",
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wordcloud in /home/hadoop/.local/lib/python2.7/site-packages (1.6.0)\n",
      "Requirement already satisfied: pillow in /home/hadoop/.local/lib/python2.7/site-packages (from wordcloud) (6.2.2)\n",
      "Requirement already satisfied: matplotlib in /home/hadoop/.local/lib/python2.7/site-packages (from wordcloud) (2.2.5)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib64/python2.7/site-packages (from wordcloud) (1.14.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib->wordcloud) (1.14.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: subprocess32 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib->wordcloud) (3.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: backports.functools-lru-cache in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib->wordcloud) (1.6.1)\n",
      "Requirement already satisfied: pytz in /home/hadoop/.local/lib/python2.7/site-packages (from matplotlib->wordcloud) (2019.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python2.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (36.2.7)\n"
     ]
    }
   ],
   "source": [
    "#!pip install seaborn\n",
    "#!pip install matplotlib\n",
    "!pip install plotly==4.6.0\n",
    "!pip install sklearn\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import countDistinct\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import datetime\n",
    "\n",
    "#import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sql_sc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+------------+------------------+-----------+----------+--------+\n",
      "|_c0|             authors|               title|  publish_timestamp|         description|                text|                 url|title_length|description_length|text_length|      data|    time|\n",
      "+---+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+------------+------------------+-----------+----------+--------+\n",
      "|  1|        ['Cbc News']|Coronavirus a 'wa...|2020-03-27 08:00:00|Canadian pharmaci...|Canadian pharmaci...|https://www.cbc.c...|          66|               248|       2600|2020-03-27|08:00:00|\n",
      "|  2|        ['Cbc News']|Yukon gov't names...|2020-03-27 01:45:00|The Yukon governm...|The Yukon governm...|https://www.cbc.c...|          60|               168|       1843|2020-03-27|01:45:00|\n",
      "|  3|['The Associated ...|U.S. Senate passe...|2020-03-26 05:13:00|The Senate has pa...|The Senate late W...|https://www.cbc.c...|          49|               172|       6185|2020-03-26|05:13:00|\n",
      "|  4|        ['Cbc News']|Coronavirus: The ...|2020-03-27 00:36:00|Scientists around...|Scientists around...|https://www.cbc.c...|          65|               159|       5011|2020-03-27|00:36:00|\n",
      "|  5|        ['Cbc News']|The latest on the...|2020-03-26 20:57:00|The latest on the...|    Trudeau says ...|https://www.cbc.c...|          51|                75|       8807|2020-03-26|20:57:00|\n",
      "|  6|['Mark Gollom Is ...|'Worse' pandemic ...|2020-03-27 08:00:00|The continued exi...|The continued exi...|https://www.cbc.c...|          68|               241|       6474|2020-03-27|08:00:00|\n",
      "|  7|        ['Cbc News']|What you need to ...|2020-03-27 08:00:00|CBC Ottawa's late...|Recent developmen...|https://www.cbc.c...|          66|                74|      11899|2020-03-27|08:00:00|\n",
      "|  8|['The Associated ...|Michigan hospital...|2020-03-26 11:02:00|Michigan hospital...|Michigan hospital...|https://www.cbc.c...|          59|               101|        856|2020-03-26|11:02:00|\n",
      "|  9| ['Thomson Reuters']|U.S. coronavirus ...|2020-03-26 14:55:00|The number of con...|The number of con...|https://www.cbc.c...|          47|               250|       8876|2020-03-26|14:55:00|\n",
      "| 10|['Leah Hendry Is ...|'Avoid the emerge...|2020-03-27 08:00:00|Montreal's Jewish...|The Jewish Genera...|https://www.cbc.c...|         109|               196|       3995|2020-03-27|08:00:00|\n",
      "+---+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+------------+------------------+-----------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.format('csv').option(\"escape\",\"\\\"\").option('header',True).load(\"s3://msbx5420-2020/team_castle_peak/news_cleaned.csv/\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'authors', 'title', 'publish_timestamp', 'description', 'text', 'url', 'title_length', 'description_length', 'text_length', 'data', 'time']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('_c0','url','time','publish_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "def cleanColumn(tmpdf,colName,findChar,replaceChar):\n",
    "    tmpdf = tmpdf.withColumn(colName, regexp_replace(colName, findChar, replaceChar))\n",
    "    return tmpdf\n",
    "\n",
    "allColNames = df.schema.names\n",
    "charToRemove= \"[\\\"!@#$%^&*\\(\\)\\{\\}\\[\\]\\'\\'\"\"',.?/:;-=+`~'...''..']\"\n",
    "replaceWith =\"\"\n",
    "for colName in allColNames:\n",
    "    df=cleanColumn(df,colName,charToRemove,replaceWith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+------------+------------------+-----------+----------+\n",
      "|             authors|               title|         description|                text|title_length|description_length|text_length|      data|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------+------------------+-----------+----------+\n",
      "|            Cbc News|Coronavirus a wak...|Canadian pharmaci...|Canadian pharmaci...|          66|               248|       2600|2020-03-27|\n",
      "|            Cbc News|Yukon govt names ...|The Yukon governm...|The Yukon governm...|          60|               168|       1843|2020-03-27|\n",
      "|The Associated Press|US Senate passes ...|The Senate has pa...|The Senate late W...|          49|               172|       6185|2020-03-26|\n",
      "|            Cbc News|Coronavirus The l...|Scientists around...|Scientists around...|          65|               159|       5011|2020-03-27|\n",
      "|            Cbc News|The latest on the...|The latest on the...|    Trudeau says ...|          51|                75|       8807|2020-03-26|\n",
      "|Mark Gollom Is A ...|Worse pandemic on...|The continued exi...|The continued exi...|          68|               241|       6474|2020-03-27|\n",
      "|            Cbc News|What you need to ...|CBC Ottawas lates...|Recent developmen...|          66|                74|      11899|2020-03-27|\n",
      "|The Associated Press|Michigan hospital...|Michigan hospital...|Michigan hospital...|          59|               101|        856|2020-03-26|\n",
      "|     Thomson Reuters|US coronavirus ca...|The number of con...|The number of con...|          47|               250|       8876|2020-03-26|\n",
      "|Leah Hendry Is A ...|Avoid the emergen...|Montreals Jewish ...|The Jewish Genera...|         109|               196|       3995|2020-03-27|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------+------------------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"year\")\n",
    "df.createOrReplaceTempView(\"Author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Author_Count|\n",
      "+------------+\n",
      "|         261|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_author = spark.sql(\"SELECT COUNT (DISTINCT authors) AS Author_Count FROM Author\")\n",
    "df_author.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Author_Count|\n",
      "+------------+\n",
      "|          78|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_year = spark.sql(\"SELECT COUNT (DISTINCT data) AS Author_Count FROM year where data LIKE '2020%'\")\n",
    "df_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Author_Count|\n",
      "+------------+\n",
      "|          78|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_year_2020 = spark.sql(\"SELECT data from year where data LIKE '2020%'\")\n",
    "df_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count: 2012324\n",
      "Unique word count: 80353\n",
      "[('are', 16368), ('shortages', 152), ('recognizing', 16), ('active', 88), ('ingredients', 30), ('have', 13226), ('of', 47822), ('complying', 24), ('amount', 189), ('medications', 75)]\n"
     ]
    }
   ],
   "source": [
    "text_rdd = df.select('text').rdd.map(list)\n",
    "splitted_word_rdd = text_rdd.flatMap(lambda line: line[0].split(' '))\n",
    "print('Total word count:', splitted_word_rdd.count())\n",
    "word_map_rdd = splitted_word_rdd.map(lambda word: (word, 1))\n",
    "word_count_rdd = word_map_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print('Unique word count:', word_count_rdd.count())\n",
    "word_count_list = word_count_rdd.collect()\n",
    "print(word_count_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>are</td>\n",
       "      <td>16368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shortages</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recognizing</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>active</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ingredients</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>have</td>\n",
       "      <td>13226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>of</td>\n",
       "      <td>47822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>complying</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>amount</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>medications</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1\n",
       "0          are  16368\n",
       "1    shortages    152\n",
       "2  recognizing     16\n",
       "3       active     88\n",
       "4  ingredients     30\n",
       "5         have  13226\n",
       "6           of  47822\n",
       "7    complying     24\n",
       "8       amount    189\n",
       "9  medications     75"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 37308)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 269, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 717, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pdDF = pd.DataFrame(word_count_list)\n",
    "pdDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
